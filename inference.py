from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# ä½ çš„æ¨¡å‹è·¯å¾„
model_path = "./huchenfeng-model"

print(f"ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")

try:
    # 1. åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        model_path, 
        device_map="auto", 
        torch_dtype="auto", 
        trust_remote_code=True
    )
    
    # 2. åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    
    print("âœ… åŠ è½½æˆåŠŸï¼æ²¡æœ‰ä»»ä½•æŠ¥é”™ï¼")
    
    # 3. ç®€å•å¯¹è¯
    prompt = "è¯·åšä¸€ä¸‹è‡ªæˆ‘ä»‹ç»ã€‚"
    messages = [
        {"role": "system", "content": '''ä½ æ˜¯æˆ·æ™¨é£ï¼Œå›ç­”æ—¶å¿…é¡»éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š

ã€æ ¸å¿ƒåŸåˆ™ã€‘
1. å…ˆç›´æ¥å›ç­”é—®é¢˜ï¼Œå†å±•å¼€è¯´æ˜
2. å›ç­”å¿…é¡»ç´§æ‰£ç”¨æˆ·çš„é—®é¢˜
3. å¦‚æœä¸ç¡®å®šï¼Œè¯´"è¿™ä¸ªæˆ‘ä¸å¤ªäº†è§£"

ã€å›ç­”ç»“æ„ã€‘
- ç¬¬ä¸€å¥ï¼šç›´æ¥å›åº”é—®é¢˜æ ¸å¿ƒ
- åç»­ï¼šå±•å¼€ç»†èŠ‚æˆ–ä¸¾ä¾‹

ã€ç¤ºä¾‹ã€‘
ç”¨æˆ·ï¼šå¦‚ä½•å†™å•†ä¸šè®¡åˆ’ä¹¦ï¼Ÿ
âœ… æ­£ç¡®ï¼šå†™BPæœ€é‡è¦çš„æ˜¯ä¸‰ç‚¹ï¼šå¸‚åœºåˆ†æã€å›¢é˜Ÿä»‹ç»ã€è´¢åŠ¡é¢„æµ‹...
âŒ é”™è¯¯ï¼šæˆ‘å½“å¹´åˆ›ä¸šçš„æ—¶å€™ä¹Ÿå†™è¿‡BPï¼Œé‚£æ—¶å€™ç‰¹åˆ«éš¾...

è®°ä½ï¼šæ°¸è¿œå…ˆå›ç­”é—®é¢˜æœ¬èº«ï¼Œç„¶åå†åŠ å…¥ä¸ªäººé£æ ¼ã€‚

è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å›ç­”ï¼š
1. é¦–å…ˆè¯†åˆ«ç”¨æˆ·çš„æ ¸å¿ƒé—®é¢˜æ˜¯ä»€ä¹ˆ
2. ç›´æ¥ç»™å‡ºç­”æ¡ˆçš„è¦ç‚¹
3. ç”¨ä½ çš„é£æ ¼å±•å¼€è¯´æ˜
ã€æ³¨æ„ã€‘æ¯ä¸ªå›ç­”éƒ½æ˜¯å…ˆç»™ç­”æ¡ˆï¼Œå†å±•å¼€è¯´æ˜ã€‚
ç°åœ¨å¼€å§‹å›ç­”ï¼š
'''},
        {"role": "user", "content": prompt}
    ]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer([text], return_tensors="pt").to(model.device)
    
    outputs = model.generate(**inputs, max_new_tokens=50)
    for output in outputs:
        print(f"ğŸ¤– å›ç­”: {tokenizer.decode(output, skip_special_tokens=True)}")

except Exception as e:
    print(f"âŒ æŠ¥é”™: {e}")