---
language:
  - zh
license: other
library_name: transformers
pipeline_tag: text-generation
tags:
  - qwen
  - lora
  - dialogue
  - chinese
---

# Model Card: HuChenFeng Qwen2.5-7B LoRA

## Model Details
- **Model type:** Conversational LLM fine-tuned to emulate the speaking style of the Chinese streamer æˆ·æ™¨é£.
- **Base model:** [Qwen2.5-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct).
- **Adaptation method:** LoRA (r=16, alpha=16) with 4-bit NormalFloat quantization, trained via [Unsloth](https://github.com/unslothai/unsloth) + SFT.
- **Training hardware:** Single RTX 4090 (â‰ˆ7 hours total fine-tuning time).
- **Intended maintainers:** HuChenFeng project authors (see [GitHub repo](https://github.com/tinymindkin/huchenfeng)).

## Model Sources
- **Repository:** https://github.com/tinymindkin/huchenfeng
- **Dataset location:** Provided alongside the repository under `dataset/` (ChatML JSON format).

## Uses
### å­¦æœ¯/ç ”ç©¶ï¼ˆFor studyï¼‰
- ä¸ºç ”ç©¶è€…æ¢ç´¢äººè®¾çº¦æŸã€å®‰å…¨æç¤ºæˆ–è®°å¿†æ¨¡å—å¦‚ä½•å½±å“é«˜åº¦é£æ ¼åŒ–åŠ©æ‰‹çš„æç¤ºå·¥ç¨‹æä¾›å®éªŒåœºã€‚
- å¯ä½œä¸ºè¦†ç›– LoRAã€QLoRA ä»¥åŠä¸­æ–‡å¯¹è¯è¯„æµ‹è¯¾ç¨‹æˆ–æ•™ç¨‹çš„åŸºçº¿æ¨¡å‹æ£€æŸ¥ç‚¹ã€‚

### å¨±ä¹/åˆ›æ„ï¼ˆFor funï¼‰
- é€‚åˆè§’è‰²èŠå¤©æœºå™¨äººã€ç›´æ’­æ­æ¡£æ¨¡æ‹Ÿæˆ–éœ€è¦å¤¸å¼ é•¿ç¯‡å™è¿°çš„äº’åŠ¨å°è¯´åº”ç”¨ã€‚
- æ”¯æŒâ€œæˆ·æ™¨é£æœºå™¨äººâ€ç­‰ç¤¾ç¾¤æŒ‘æˆ˜æˆ–è§’è‰²æ‰®æ¼”æ´»åŠ¨ï¼Œæ–¹ä¾¿ç²‰ä¸æ”¹ç¼–å°è¯ä¸å³å…´å‘æŒ¥ã€‚


## Dataset
- **Size:** 80,137 dialogue pairs; average question length 18.3 Chinese characters, average answer length 342.7 characters; â‰ˆ2.8B tokens (Qwen tokenizer).
- **Source:** 2023-2024 livestream transcripts (>200 words per utterance) collected from æˆ·æ™¨é£â€™s streams.
- **Processing pipeline:**
  1. Whisper Large-v3 for speech-to-text transcription (~2M words raw).
  2. Gemini-2.5-Flash cleaning (remove short segments, repeated content, background chatter, comment reading, obvious ASR errors) with cost â‰ˆ \$42.
  3. Gemini-2.5-Flash-Lite prompt-driven question generation (3â€“5 prompts per segment) with cost â‰ˆ \$18.
  4. Post-filtering for length, duplication, sentiment; final 80K ChatML-format pairs released publicly, with 12K high-confidence entries used for fine-tuning.
- **Format:** ChatML-style JSON with alternating `user` and `assistant` messages.

## Training Procedure
### Hyperparameters
```yaml
learning_rate: 2e-4
batch_size: 4 (physical) x 4 (grad accumulation) = 16 effective
epochs: 3
optimizer: AdamW-8bit
warmup_steps: 10
max_seq_length: 2048
total_steps: ~2250
```
- LoRA: r=16, alpha=16, dropout default (0.1). Adapter layers applied to attention projections.
- Quantization: 4-bit NormalFloat (bitsandbytes) to reduce VRAM consumption.

### Data Sampling
- Length-based stratified sampling to preserve mix of short and long responses.
- Deduplication threshold 60% to avoid near-identical segments.
- Manual spot-checking to keep persona-consistent segments only.

### Training Infrastructure
- Single RTX 4090 (24 GB VRAM) using Unsloth accelerated fine-tuning.
- Gradient checkpointing enabled; logging via TRL.


## How to Use
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_path = "YOUR MODEL PATH"
# 1. load model
model = AutoModelForCausalLM.from_pretrained(
    model_path, 
    device_map="auto", 
    torch_dtype="auto", 
    trust_remote_code=True
)

# 2. load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

prompt = "ä½ æ€ä¹ˆçœ‹å¾…å¤§ä¸“æ¯•ä¸šçš„èŒä¸šé€‰æ‹©ï¼Ÿ"
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
 prompt = "è¯·åšä¸€ä¸‹è‡ªæˆ‘ä»‹ç»ã€‚"
    messages = [
        {"role": "system", "content": '''ä½ æ˜¯æˆ·æ™¨é£ï¼Œå›ç­”æ—¶å¿…é¡»éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š

ã€æ ¸å¿ƒåŸåˆ™ã€‘
1. å…ˆç›´æ¥å›ç­”é—®é¢˜ï¼Œå†å±•å¼€è¯´æ˜
2. å›ç­”å¿…é¡»ç´§æ‰£ç”¨æˆ·çš„é—®é¢˜
3. å¦‚æœä¸ç¡®å®šï¼Œè¯´"è¿™ä¸ªæˆ‘ä¸å¤ªäº†è§£"
'''},
{"role": "user", "content": prompt}
]
text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = tokenizer([text], return_tensors="pt").to(model.device)

outputs = model.generate(**inputs, max_new_tokens=50)
for output in outputs:
print(f"ğŸ¤– å›ç­”: {tokenizer.decode(output, skip_special_tokens=True)}")
```


## Contact
GitHub: https://github.com/tinymindkin/huchenfeng
