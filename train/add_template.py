from transformers import AutoTokenizer
import os

model_path = "./merged_model_final"

print(f"ğŸ”§ æ­£åœ¨ä¿®å¤ {model_path} çš„å¯¹è¯æ¨¡æ¿...")

# 1. åŠ è½½æœ¬åœ°çš„åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

# 2. Qwen2.5 çš„æ ‡å‡† ChatML æ¨¡æ¿
# è¿™æ˜¯ä¸€æ®µ Jinja2 ä»£ç ï¼Œå‘Šè¯‰åˆ†è¯å™¨æ€ä¹ˆå¤„ç† system/user/assistant æ¶ˆæ¯
qwen_chat_template = (
    "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}"
    "{% for message in messages %}"
    "{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}"
    "{% endfor %}"
    "{% if add_generation_prompt %}"
    "{{ '<|im_start|>assistant\n' }}"
    "{% endif %}"
)

# 3. å°†æ¨¡æ¿èµ‹å€¼ç»™åˆ†è¯å™¨
tokenizer.chat_template = qwen_chat_template

# 4. ä¿å­˜å›æœ¬åœ° (è¦†ç›–æ—§çš„ tokenizer_config.json)
print("ğŸ’¾ æ­£åœ¨ä¿å­˜ä¿®å¤åçš„é…ç½®...")
tokenizer.save_pretrained(model_path)

print("âœ… ä¿®å¤å®Œæˆï¼ç°åœ¨çš„æ¨¡å‹æ–‡ä»¶å¤¹å·²ç»åŒ…å«äº†æ­£ç¡®çš„ chat_templateã€‚")
print("è¯·å†æ¬¡è¿è¡Œ python test.py")